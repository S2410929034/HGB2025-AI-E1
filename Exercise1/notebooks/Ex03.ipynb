{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087d7171-b945-4e95-865a-92c2ab33d6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading people_big from PostgreSQL ===\n",
      "Rows loaded: 1000000\n",
      "Load time: 4.77 seconds\n",
      "\n",
      "=== Query (a): AVG salary per department ===\n",
      "+------------------+----------+\n",
      "|department        |avg_salary|\n",
      "+------------------+----------+\n",
      "|Workforce Planning|85090.82  |\n",
      "|Web Development   |84814.36  |\n",
      "|UX Design         |84821.2   |\n",
      "|UI Design         |85164.64  |\n",
      "|Treasury          |84783.27  |\n",
      "|Training          |85148.1   |\n",
      "|Tax               |85018.57  |\n",
      "|Sustainability    |85178.99  |\n",
      "|Supply Chain      |84952.89  |\n",
      "|Subscriptions     |84899.19  |\n",
      "+------------------+----------+\n",
      "\n",
      "Query (a) time: 3.74 seconds\n",
      "\n",
      "=== Query (b): Nested aggregation ===\n",
      "+------------+-----------------+\n",
      "|country     |avg_salary       |\n",
      "+------------+-----------------+\n",
      "|Egypt       |87382.229633112  |\n",
      "|Kuwait      |87349.3517377211 |\n",
      "|Saudi Arabia|87348.80512175433|\n",
      "|Panama      |87345.00623707911|\n",
      "|Denmark     |87328.03514120901|\n",
      "|Jamaica     |87305.437352083  |\n",
      "|Lebanon     |87292.76891750695|\n",
      "|Turkey      |87290.69043798617|\n",
      "|Malaysia    |87253.78746341489|\n",
      "|Kazakhstan  |87251.74274968785|\n",
      "+------------+-----------------+\n",
      "\n",
      "Query (b) time: 3.17 seconds\n",
      "\n",
      "=== Query (c): Top 10 salaries ===\n",
      "+------+----------+---------+------+----------------+------+------------+\n",
      "|id    |first_name|last_name|gender|department      |salary|country     |\n",
      "+------+----------+---------+------+----------------+------+------------+\n",
      "|764650|Tim       |Jensen   |Male  |Analytics       |160000|Bulgaria    |\n",
      "|10016 |Anastasia |Edwards  |Female|Analytics       |159998|Kuwait      |\n",
      "|754528|Adrian    |Young    |Male  |Game Analytics  |159997|UK          |\n",
      "|240511|Diego     |Lopez    |Male  |Game Analytics  |159995|Malaysia    |\n",
      "|893472|Mariana   |Cook     |Female|People Analytics|159995|South Africa|\n",
      "|359891|Mariana   |Novak    |Female|Game Analytics  |159992|Mexico      |\n",
      "|53102 |Felix     |Taylor   |Male  |Data Science    |159989|Bosnia      |\n",
      "|768143|Teresa    |Campbell |Female|Game Analytics  |159988|Spain       |\n",
      "|729165|Antonio   |Weber    |Male  |Analytics       |159987|Moldova     |\n",
      "|952549|Adrian    |Harris   |Male  |Analytics       |159986|Georgia     |\n",
      "+------+----------+---------+------+----------------+------+------------+\n",
      "\n",
      "Query (c) time: 3.3 seconds\n",
      "\n",
      "=== Query (d): Heavy self-join COUNT (DANGEROUS) ===\n",
      "Join count: 10983941260\n",
      "Query (d) time: 17.17 seconds\n",
      "\n",
      "=== Query (d-safe): Join-equivalent rewrite ===\n",
      "+-----------+\n",
      "|total_pairs|\n",
      "+-----------+\n",
      "|10983941260|\n",
      "+-----------+\n",
      "\n",
      "Query (d-safe) time: 1.44 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Spark session\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import builtins  # <-- IMPORTANT\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg,\n",
    "    round as spark_round,   # Spark round ONLY for Columns\n",
    "    count,\n",
    "    col,\n",
    "    sum as _sum\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PostgresVsSparkBenchmark\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.2\")\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.history.fs.logDirectory\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ============================================\n",
    "# 1. JDBC connection config\n",
    "# ============================================\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 2. Load data from PostgreSQL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Loading people_big from PostgreSQL ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_big = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"people_big\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "# Force materialization\n",
    "row_count = df_big.count()\n",
    "\n",
    "print(f\"Rows loaded: {row_count}\")\n",
    "print(\"Load time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Register temp view\n",
    "df_big.createOrReplaceTempView(\"people_big\")\n",
    "\n",
    "# ============================================\n",
    "# 3. Query (a): Simple aggregation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (a): AVG salary per department ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_a = (\n",
    "    df_big\n",
    "    .groupBy(\"department\")\n",
    "    .agg(spark_round(avg(\"salary\"), 2).alias(\"avg_salary\"))\n",
    "    .orderBy(\"department\", ascending=False)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "q_a.collect()\n",
    "q_a.show(truncate=False)\n",
    "print(\"Query (a) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Query (b): Nested aggregation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (b): Nested aggregation ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = spark.sql(\"\"\"\n",
    "SELECT country, AVG(avg_salary) AS avg_salary\n",
    "FROM (\n",
    "    SELECT country, department, AVG(salary) AS avg_salary\n",
    "    FROM people_big\n",
    "    GROUP BY country, department\n",
    ") sub\n",
    "GROUP BY country\n",
    "ORDER BY avg_salary DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query (b) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Query (c): Sorting + Top-N\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (c): Top 10 salaries ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_c = (\n",
    "    df_big\n",
    "    .orderBy(col(\"salary\").desc())\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "q_c.collect()\n",
    "q_c.show(truncate=False)\n",
    "print(\"Query (c) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Query (d): Heavy self-join (COUNT only)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (d): Heavy self-join COUNT (DANGEROUS) ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_d = (\n",
    "    df_big.alias(\"p1\")\n",
    "    .join(df_big.alias(\"p2\"), on=\"country\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(\"Join count:\", q_d)\n",
    "print(\"Query (d) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 7. Query (d-safe): Join-equivalent rewrite\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (d-safe): Join-equivalent rewrite ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "grouped = df_big.groupBy(\"country\").agg(count(\"*\").alias(\"cnt\"))\n",
    "\n",
    "q_d_safe = grouped.select(\n",
    "    _sum(col(\"cnt\") * col(\"cnt\")).alias(\"total_pairs\")\n",
    ")\n",
    "\n",
    "q_d_safe.collect()\n",
    "q_d_safe.show()\n",
    "print(\"Query (d-safe) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 8. Cleanup\n",
    "# ============================================\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687a68b9-7d7f-4e2c-bfb7-3b677f0c8e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading orders_big from PostgreSQL ===\n",
      "Rows loaded: 1000000\n",
      "Load time: 1.57 seconds\n",
      "\n",
      "=== Query (A): Max price_per_unit ===\n",
      "+-------------+----------------+--------------+\n",
      "|customer_name|product_category|price_per_unit|\n",
      "+-------------+----------------+--------------+\n",
      "|Emma Brown   |Automotive      |2000.0        |\n",
      "+-------------+----------------+--------------+\n",
      "\n",
      "Query (A) time: 2.67 seconds\n",
      "\n",
      "=== Query (B): Top 3 categories by quantity ===\n",
      "+----------------+--------------+\n",
      "|product_category|total_quantity|\n",
      "+----------------+--------------+\n",
      "|Health & Beauty |300842        |\n",
      "|Electronics     |300804        |\n",
      "|Toys            |300598        |\n",
      "+----------------+--------------+\n",
      "\n",
      "Query (B) time: 1.54 seconds\n",
      "\n",
      "=== Query (C): Total revenue per category ===\n",
      "+----------------+--------------------+\n",
      "|product_category|total_revenue       |\n",
      "+----------------+--------------------+\n",
      "|Automotive      |3.065897988599943E8 |\n",
      "|Electronics     |2.4152500945000267E8|\n",
      "|Home & Garden   |7.80237800900001E7  |\n",
      "|Sports          |6.1848990830000326E7|\n",
      "|Health & Beauty |4.65998178900003E7  |\n",
      "|Office Supplies |3.8276061640000574E7|\n",
      "|Fashion         |3.1566368219999947E7|\n",
      "|Toys            |2.3271039019999716E7|\n",
      "|Grocery         |1.5268355660000028E7|\n",
      "|Books           |1.273197603999989E7 |\n",
      "+----------------+--------------------+\n",
      "\n",
      "Query (C) time: 2.3 seconds\n",
      "\n",
      "=== Query (D): Top customers by total spending ===\n",
      "+--------------+-----------------+\n",
      "|customer_name |total_spent      |\n",
      "+--------------+-----------------+\n",
      "|Carol Taylor  |991179.1800000003|\n",
      "|Nina Lopez    |975444.9499999998|\n",
      "|Daniel Jackson|959344.4800000001|\n",
      "|Carol Lewis   |947708.5700000002|\n",
      "|Daniel Young  |946030.1400000004|\n",
      "|Alice Martinez|935100.0199999999|\n",
      "|Ethan Perez   |934841.2399999991|\n",
      "|Leo Lee       |934796.4799999993|\n",
      "|Eve Young     |933176.8599999989|\n",
      "|Ivy Rodriguez |925742.6400000005|\n",
      "+--------------+-----------------+\n",
      "\n",
      "Query (D) time: 2.09 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0. Imports & Spark session\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import builtins  # <-- IMPORTANT\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg,\n",
    "    round as spark_round,   # Spark round ONLY for Columns\n",
    "    count,\n",
    "    col,\n",
    "    sum as _sum\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PostgresVsSparkBenchmark\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.2\")\n",
    "    .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    .config(\"spark.eventLog.dir\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.history.fs.logDirectory\", \"/tmp/spark-events\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# ============================================\n",
    "# 1. JDBC connection config\n",
    "# ============================================\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# ============================================\n",
    "# 2. Load data from PostgreSQL\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Loading orders_big from PostgreSQL ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_big = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"orders_big\",\n",
    "    properties=jdbc_props\n",
    ")\n",
    "\n",
    "# Force materialization\n",
    "row_count = df_big.count()\n",
    "\n",
    "print(f\"Rows loaded: {row_count}\")\n",
    "print(\"Load time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# Register temp view\n",
    "df_big.createOrReplaceTempView(\"orders_big\")\n",
    "\n",
    "# ============================================\n",
    "# 3. A: Single item with highest price_per_unit\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (A): Max price_per_unit ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_a = (\n",
    "    df_big.select(\"customer_name\", \"product_category\", \"price_per_unit\")\n",
    "          .orderBy(col(\"price_per_unit\").desc())\n",
    "          .limit(1)\n",
    ")\n",
    "\n",
    "q_a.collect()\n",
    "q_a.show(truncate=False)\n",
    "print(\"Query (A) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 4. B: Top 3 product categories by total quantity\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (B): Top 3 categories by quantity ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_b = (\n",
    "    df_big.groupBy(\"product_category\")\n",
    "          .agg(_sum(\"quantity\").alias(\"total_quantity\"))\n",
    "          .orderBy(col(\"total_quantity\").desc())\n",
    "          .limit(3)\n",
    ")\n",
    "\n",
    "q_b.collect()\n",
    "q_b.show(truncate=False)\n",
    "print(\"Query (B) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 5. C: Total revenue per product category\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (C): Total revenue per category ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_c = (\n",
    "    df_big.groupBy(\"product_category\")\n",
    "          .agg(_sum(col(\"price_per_unit\") * col(\"quantity\")).alias(\"total_revenue\"))\n",
    "          .orderBy(col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "q_c.collect()\n",
    "q_c.show(truncate=False)\n",
    "print(\"Query (C) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 6. D: Customers with highest total spending\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n=== Query (D): Top customers by total spending ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_d = (\n",
    "    df_big.groupBy(\"customer_name\")\n",
    "          .agg(_sum(col(\"price_per_unit\") * col(\"quantity\")).alias(\"total_spent\"))\n",
    "          .orderBy(col(\"total_spent\").desc())\n",
    "          .limit(10)\n",
    ")\n",
    "\n",
    "q_d.collect()\n",
    "q_d.show(truncate=False)\n",
    "print(\"Query (D) time:\", builtins.round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "# ============================================\n",
    "# 7. Cleanup\n",
    "# ============================================\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e4165-8a64-42cd-8d78-4bae02b839a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
